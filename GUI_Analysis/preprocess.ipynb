{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "# ...existing code...\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cache_embeddings = \"./cache_embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_dir = \"path/to/clip/model\"  # specify the path to your CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(clip_dir).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_data_dir(data_dir: str, dataset: str) -> Path:\n",
    "    \"\"\"\n",
    "    Map original data_dir to corresponding directory under cache_embeddings\n",
    "    data_dir example:\n",
    "      /root/.../osworld/ui_tars_15_7b/chrome/xxxx\n",
    "    Returns:\n",
    "      ./cache_embeddings/osworld/ui_tars_15_7b/chrome/xxxx\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    parts = list(data_dir.parts)\n",
    "    if dataset not in parts:\n",
    "        raise ValueError(f\"Dataset '{dataset}' not found in data_dir '{data_dir}'\")\n",
    "    idx = parts.index(dataset)\n",
    "    rel_from_dataset = Path(*parts[idx:])             # osworld/...\n",
    "    cache_root = Path(dir_cache_embeddings)\n",
    "    return cache_root / rel_from_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53babec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            # Preprocess image to get pixel_values\n",
    "            # Note: processing single image here, returns tensor\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].squeeze(0) # [3, 224, 224]\n",
    "            return pixel_values, idx, True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a placeholder, marked as invalid\n",
    "            return torch.zeros(3, 224, 224), idx, False\n",
    "\n",
    "def compute_clip_embeddings_batch(image_paths: list, save_paths: list, batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Use DataLoader multiprocessing to accelerate batch computation of CLIP embeddings\n",
    "    \"\"\"\n",
    "    if len(image_paths) != len(save_paths):\n",
    "        raise ValueError(\"image_paths and save_paths must have the same length\")\n",
    "    \n",
    "    # 1. Filter out already existing ones\n",
    "    to_process_indices = []\n",
    "    for i, save_path in enumerate(save_paths):\n",
    "        if not save_path.exists():\n",
    "            to_process_indices.append(i)\n",
    "    \n",
    "    if not to_process_indices:\n",
    "        return\n",
    "\n",
    "    # Extract paths that need processing\n",
    "    current_image_paths = [image_paths[i] for i in to_process_indices]\n",
    "    current_save_paths = [save_paths[i] for i in to_process_indices]\n",
    "\n",
    "    # 2. Build Dataset and DataLoader\n",
    "    # num_workers recommended to be set to CPU core count, e.g., 8 or 16\n",
    "    num_workers = 32\n",
    "    dataset = ImageDataset(current_image_paths, clip_processor)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    # 3. Batch inference\n",
    "    # Using inference_mode is slightly faster than no_grad\n",
    "    with torch.inference_mode():\n",
    "        for batch_pixels, batch_indices, batch_valid in tqdm(dataloader, desc=\"Computing embeddings\", leave=False):\n",
    "            \n",
    "            # Filter out images that failed to load\n",
    "            valid_mask = batch_valid.bool()\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "            \n",
    "            # Move to GPU\n",
    "            pixel_values = batch_pixels[valid_mask].to(device, non_blocking=True)\n",
    "            \n",
    "            # Compute features\n",
    "            image_features = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "            \n",
    "            # L2 normalization\n",
    "            image_features = torch.nn.functional.normalize(image_features, p=2, dim=-1)\n",
    "            \n",
    "            # Move back to CPU for saving\n",
    "            image_features = image_features.cpu()\n",
    "            \n",
    "            # Save\n",
    "            # Note: batch_indices are indices in the original dataset\n",
    "            current_batch_indices = batch_indices[valid_mask]\n",
    "            \n",
    "            for i, feat in enumerate(image_features):\n",
    "                global_idx = current_batch_indices[i].item()\n",
    "                save_path = current_save_paths[global_idx]\n",
    "                \n",
    "                save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(feat.unsqueeze(0), save_path)\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traj_jsonl(input_jsonl: str, batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Preprocess jsonl generated by transform_osworld_trajectories:\n",
    "      - Generate CLIP embeddings (.pt) for each observation image\n",
    "      - Replace observation field with corresponding .pt file path\n",
    "      - Output to dir_cache_embeddings/dataset/model\n",
    "    \n",
    "    Args:\n",
    "        input_jsonl: Input jsonl file path\n",
    "        batch_size: Batch size for CLIP encoding\n",
    "    \"\"\"\n",
    "    input_path = Path(input_jsonl)\n",
    "\n",
    "    # Read all trajectories\n",
    "    trajectories = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            trajectories.append(json.loads(line))\n",
    "\n",
    "    if not trajectories:\n",
    "        print(\"No trajectories read\")\n",
    "        return\n",
    "\n",
    "    dataset = trajectories[0][\"dataset\"]\n",
    "    model_name = trajectories[0][\"model\"]\n",
    "\n",
    "    # Output jsonl placed under dir_cache_embeddings/dataset/model\n",
    "    out_dir = Path(dir_cache_embeddings) / dataset / model_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / input_path.name\n",
    "\n",
    "    print(f\"dataset: {dataset}, model: {model_name}\")\n",
    "    print(f\"Output trajectory file: {out_path}\")\n",
    "\n",
    "    # First pass: collect all images that need processing\n",
    "    all_image_paths = []\n",
    "    all_save_paths = []\n",
    "    image_to_embed_map = {}  # Original image path -> embedding file path\n",
    "    \n",
    "    print(\"Collecting all image paths...\")\n",
    "    for traj in tqdm(trajectories, desc=\"Collecting images\"):\n",
    "        data_dir = traj[\"data_dir\"]\n",
    "        cache_data_dir = get_cache_data_dir(data_dir, dataset)\n",
    "\n",
    "        for step in traj.get(\"trajectory\", []):\n",
    "            obs = step.get(\"observation\", \"\")\n",
    "            if not obs or obs == \"empty\":\n",
    "                continue\n",
    "\n",
    "            obs_path = Path(obs)\n",
    "            if not obs_path.is_file():\n",
    "                continue\n",
    "\n",
    "            # Calculate embedding file path\n",
    "            try:\n",
    "                rel_obs = obs_path.relative_to(Path(data_dir))\n",
    "            except ValueError:\n",
    "                rel_obs = obs_path.name\n",
    "\n",
    "            embed_dir = cache_data_dir / rel_obs.parent\n",
    "            embed_path = embed_dir / (Path(rel_obs).stem + \".pt\")\n",
    "\n",
    "            # Record mapping relationship\n",
    "            image_to_embed_map[str(obs_path)] = str(embed_path.resolve())\n",
    "            \n",
    "            # Add to batch processing list\n",
    "            if not embed_path.exists():\n",
    "                all_image_paths.append(obs_path)\n",
    "                all_save_paths.append(embed_path)\n",
    "\n",
    "    # Batch compute embeddings\n",
    "    print(f\"\\nTotal images to process: {len(all_image_paths)}\")\n",
    "    if all_image_paths:\n",
    "        compute_clip_embeddings_batch(all_image_paths, all_save_paths, batch_size=batch_size)\n",
    "\n",
    "    # Second pass: update trajectory data and save\n",
    "    print(\"\\nUpdating trajectory data...\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for traj in tqdm(trajectories, desc=\"Writing trajectories\"):\n",
    "            for step in traj.get(\"trajectory\", []):\n",
    "                obs = step.get(\"observation\", \"\")\n",
    "                if obs and obs != \"empty\" and obs in image_to_embed_map:\n",
    "                    step[\"observation\"] = image_to_embed_map[obs]\n",
    "\n",
    "            f_out.write(json.dumps(traj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nPreprocessing completed!\")\n",
    "    print(f\"Processed {len(trajectories)} trajectories\")\n",
    "    print(f\"Generated {len(all_image_paths)} embedding files\")\n",
    "    print(f\"Output file: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054afdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "def preprocess_all_jsonl_in_dataset(dataset_dir: str = \"osworld\", batch_size: int = 128):\n",
    "    \"\"\"\n",
    "    Find all *_transformed_trajectories.jsonl files under dataset directory and batch process them\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Dataset directory path\n",
    "        batch_size: Batch size for CLIP encoding\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"Error: Directory {dataset_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Find all jsonl files\n",
    "    jsonl_files = list(dataset_path.glob(\"*_transformed_trajectories.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"No *_transformed_trajectories.jsonl files found in {dataset_dir} directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(jsonl_files)} jsonl files:\")\n",
    "    for f in jsonl_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print()\n",
    "    \n",
    "    # Process one by one\n",
    "    for idx, jsonl_file in enumerate(jsonl_files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing file {idx}/{len(jsonl_files)}: {jsonl_file.name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            preprocess_traj_jsonl(str(jsonl_file), batch_size=batch_size)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: Error processing {jsonl_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All completed! Processed {len(jsonl_files)} files in total\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ecc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically process all jsonl files in osworld directory\n",
    "preprocess_all_jsonl_in_dataset(dataset_dir=\"androidworld\", batch_size=1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
