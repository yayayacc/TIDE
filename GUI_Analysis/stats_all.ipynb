{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b18a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import orjson\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_category(action_str):\n",
    "    \"\"\"\n",
    "    Classify actions based on pyautogui function name\n",
    "    \"\"\"\n",
    "    if not action_str or 'pyautogui' not in action_str:\n",
    "        return None\n",
    "    \n",
    "    # Extract function name\n",
    "    match = re.search(r'pyautogui\\.(\\w+)\\(', action_str)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    func_name = match.group(1)\n",
    "    \n",
    "    \n",
    "    return func_name\n",
    "\n",
    "def analyze_loop_action_types(base_dir):\n",
    "    \"\"\"\n",
    "    Count type distribution of loop actions in analysis.json\n",
    "    \"\"\"\n",
    "    def process_line(line):\n",
    "        if not line.strip():\n",
    "            return None\n",
    "        return orjson.loads(line)\n",
    "\n",
    "    def load_jsonl_parallel(data_path, n_workers=64):\n",
    "        with open(data_path, \"rb\") as f:\n",
    "            lines = f.readlines()\n",
    "        # Use thread pool for IO-intensive and CPU-intensive tasks that release GIL\n",
    "        with ThreadPool(n_workers) as pool:\n",
    "            data = [x for x in pool.map(process_line, lines) if x is not None]\n",
    "        return data\n",
    "    all_traj = load_jsonl_parallel(base_dir)\n",
    "    action_counts = defaultdict(int)\n",
    "    total_actions = 0\n",
    "\n",
    "\n",
    "            \n",
    "    # Iterate through all sample folders under each task\n",
    "    for traj in all_traj:\n",
    "        # Check if there are loop error records\n",
    "        trajectory= traj[\"trajectory\"]\n",
    "        for step in trajectory:\n",
    "                for action in step[\"action\"]:\n",
    "                    category = get_action_category(action)\n",
    "                    if category:\n",
    "                        action_counts[category] += 1\n",
    "                        total_actions += 1\n",
    "\n",
    "    # Calculate statistics\n",
    "    results = []\n",
    "    for category, count in action_counts.items():\n",
    "        percentage = (count / total_actions * 100) if total_actions > 0 else 0\n",
    "        results.append({\n",
    "            \"Action Type\": category,\n",
    "            \"Count\": count,\n",
    "            \"Percentage\": f\"{percentage:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(\"Count\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "root_dir = \"analysis_third_part\"\n",
    "dataset=\"waa\"\n",
    "dataset_path = Path(root_dir) / dataset\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"Dataset path not found: {dataset}\")\n",
    "else:\n",
    "    print(f\"Starting to analyze dataset: {dataset}\")\n",
    "    \n",
    "    # Data for aggregating all models\n",
    "    all_models_data = []\n",
    "    all_action_types = set()\n",
    "    \n",
    "    # Iterate through all model folders under dataset\n",
    "    for model_dir in dataset_path.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        model_name = model_dir.name\n",
    "        print(f\"\\n{'='*30}\\nProcessing model: {model_name}\")\n",
    "        \n",
    "        # Run analysis\n",
    "        all_traj_dir = f\"{dataset}/{model_name}_transformed_trajectories.jsonl\"\n",
    "        loop_stats_df = analyze_loop_action_types(all_traj_dir)\n",
    "        \n",
    "        if not loop_stats_df.empty:\n",
    "            print(f\"Loop action type statistics ({model_name}):\")\n",
    "            print(loop_stats_df)\n",
    "            \n",
    "            # Save single model result\n",
    "            output_file = f\"{dataset_path}/all_action_stats_{model_name}.csv\"\n",
    "            loop_stats_df.to_csv(output_file, index=False, sep='\\t')\n",
    "            print(f\"Results saved to: {output_file}\")\n",
    "            \n",
    "            # Collect all action types\n",
    "            for action_type in loop_stats_df['Action Type']:\n",
    "                all_action_types.add(action_type)\n",
    "            \n",
    "            # Add model name column\n",
    "            loop_stats_df['Model'] = model_name\n",
    "            all_models_data.append(loop_stats_df)\n",
    "        else:\n",
    "            print(f\"Model {model_name} has no Loop action data or directory is empty.\")\n",
    "    \n",
    "    # Aggregate all model data\n",
    "    if all_models_data:\n",
    "        # Merge all model data\n",
    "        combined_df = pd.concat(all_models_data, ignore_index=True)\n",
    "        \n",
    "        # Create pivot table, rows are models, columns are action types\n",
    "        pivot_df = combined_df.pivot_table(\n",
    "            index='Model',\n",
    "            columns='Action Type',\n",
    "            values='Count',\n",
    "            fill_value=0,\n",
    "            aggfunc='sum'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Ensure all action types are in columns\n",
    "        for action_type in all_action_types:\n",
    "            if action_type not in pivot_df.columns:\n",
    "                pivot_df[action_type] = 0\n",
    "        \n",
    "        # Calculate total and percentage for each model\n",
    "        action_columns = [col for col in pivot_df.columns if col != 'Model']\n",
    "        pivot_df['Total'] = pivot_df[action_columns].sum(axis=1)\n",
    "        \n",
    "        # Add percentage column for each action type\n",
    "        for action_type in action_columns:\n",
    "            pivot_df[f'{action_type}_pct'] = (pivot_df[action_type] / pivot_df['Total'] * 100).round(2)\n",
    "        \n",
    "        # Save aggregated results\n",
    "        summary_file = f\"{dataset_path}/all_action_stats_all_models_summary.csv\"\n",
    "        pivot_df.to_csv(summary_file, index=False, sep='\\t')\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"All models aggregated results saved to: {summary_file}\")\n",
    "        print(f\"\\nAggregated statistics:\")\n",
    "        print(pivot_df)\n",
    "        \n",
    "        # Also save raw merged data\n",
    "        raw_summary_file = f\"{dataset_path}/all_action_stats_all_models_raw.csv\"\n",
    "        combined_df.to_csv(raw_summary_file, index=False, sep='\\t')\n",
    "        print(f\"Raw merged data saved to: {raw_summary_file}\")\n",
    "    else:\n",
    "        print(\"\\nNo model data found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
