{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(base_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load result.json files from all experiment subdirectories under the specified base directory,\n",
    "    and merge them into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    all_results_data = []\n",
    "\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"Error: Base directory '{base_dir}' does not exist.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Iterate through all entries in base directory\n",
    "    for exp_folder_name in os.listdir(base_dir):\n",
    "        exp_path = os.path.join(base_dir, exp_folder_name)\n",
    "\n",
    "        # Ensure it is a directory\n",
    "        if os.path.isdir(exp_path):\n",
    "            result_file_path = os.path.join(exp_path, \"result.json\")\n",
    "\n",
    "            # Check if result.json exists\n",
    "            if os.path.exists(result_file_path):\n",
    "                try:\n",
    "                    with open(result_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        result_data = json.load(f)\n",
    "                        # Add experiment folder name as a column for identification\n",
    "                        result_data[\"experiment\"] = exp_folder_name\n",
    "                        all_results_data.append(result_data)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Unable to parse JSON in file {result_file_path}.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading file {result_file_path}: {e}\")\n",
    "\n",
    "    if not all_results_data:\n",
    "        print(\"No 'result.json' files found or processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    results_df = pd.DataFrame(all_results_data)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adff6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_area_under_curve(df: pd.DataFrame, start:int, end:int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the area under the curve (AUC) for specified metrics, subtracting the area below the minimum value in the interval.\n",
    "    Also update pass_k and pass_mean to the accuracy at the end step.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing experiment results\n",
    "        start: Starting turn number\n",
    "        end: Ending turn number\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added AUC and end accuracy columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    passk_auc = []\n",
    "    pass1_auc = []\n",
    "    passk_end_values = []\n",
    "    pass1_end_values = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        passk_dict = row['passk_at_T']\n",
    "        pass1_dict = row['pass1_at_T']\n",
    "        \n",
    "        def extract_series(data_dict):\n",
    "            values = []\n",
    "            last_value = 0.0  # Initialize value from previous round\n",
    "            for t in range(start, end + 1):\n",
    "                key = str(t)\n",
    "                if key in data_dict:\n",
    "                    last_value = data_dict[key]  # Update value from previous round\n",
    "                    values.append(last_value)\n",
    "                else:\n",
    "                    values.append(last_value)  # Use value from previous round\n",
    "            return values\n",
    "        \n",
    "        passk_values = extract_series(passk_dict)\n",
    "        pass1_values = extract_series(pass1_dict)\n",
    "        \n",
    "        min_passk = min(passk_values)\n",
    "        adjusted_passk = [value - min_passk for value in passk_values]\n",
    "        passk_auc.append(max(np.trapz(adjusted_passk, dx=1), 0.0))\n",
    "        \n",
    "        min_pass1 = min(pass1_values)\n",
    "        adjusted_pass1 = [value - min_pass1 for value in pass1_values]\n",
    "        pass1_auc.append(max(np.trapz(adjusted_pass1, dx=1), 0.0))\n",
    "        \n",
    "        def get_value_at_step(data_dict):\n",
    "            end_key = str(end)\n",
    "            if end_key in data_dict:\n",
    "                return data_dict[end_key]\n",
    "            if not data_dict:\n",
    "                return np.nan\n",
    "            numeric_keys = sorted(int(k) for k in data_dict.keys())\n",
    "            lower_keys = [k for k in numeric_keys if k <= end]\n",
    "            if lower_keys:\n",
    "                return data_dict[str(lower_keys[-1])]\n",
    "            higher_keys = [k for k in numeric_keys if k > end]\n",
    "            if higher_keys:\n",
    "                return data_dict[str(higher_keys[0])]\n",
    "            return np.nan\n",
    "        \n",
    "        passk_end_values.append(get_value_at_step(passk_dict))\n",
    "        pass1_end_values.append(get_value_at_step(pass1_dict))\n",
    "    \n",
    "    span = max(end - start, 1)  # Avoid division by zero\n",
    "    df['passk_auc'] = np.array(passk_auc, dtype=float) / span\n",
    "    df['pass1_auc'] = np.array(pass1_auc, dtype=float) / span\n",
    "    df['pass_k'] = passk_end_values\n",
    "    df['pass_mean'] = pass1_end_values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349510ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format = \"user_assistant\"\n",
    "format = \"user_assistant_format\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf005c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_criteria = {\n",
    "    # 'model_name': 'Qwen3-4B',\n",
    "    # \"enable_thinking\": False,\n",
    "    \"state\": \"env\",\n",
    "    'chat_format': f'{format}',\n",
    "    # \"alfworld_mode\": \"eval_in_distribution\",\n",
    "    'history_has_cot': True,\n",
    "    \"stop_by_self\": False,\n",
    "    # \"offer_feedback\": True,\n",
    "    # \"prompt_example\": \"fewshot\",\n",
    "    \"history_window_size\": 0,\n",
    "}\n",
    "# if format == \"user_assistant_format_part\":\n",
    "#     selection_criteria['history_window_size'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9958052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\n",
    "    'Qwen3-4B',\n",
    "    'Qwen3-30B-A3B',\n",
    "    'Llama3-8B',\n",
    "    'Llama3-70B',\n",
    "    'Llama-3.1-8B',\n",
    "    'Llama-3.3-70B',\n",
    "    'Glm-9B-Chat',\n",
    "    'Glm4-9B-Chat',\n",
    "    \"GLM-4-32B-0414\",\n",
    "    'Mistral-7B-Instruct-v0.3',\n",
    "    'Ministral-3-14B-Instruct-2512',\n",
    "    'phi-4',\n",
    "    'deepseek-v3',\n",
    "    'deepseek-v3.2',\n",
    "    'gemini-2.5-flash',\n",
    "    'gemini-2.5-flash-nothinking',\n",
    "    'Phi-4-reasoning',\n",
    "    'gpt-oss-120b',\n",
    "    'deepseek-r1',\n",
    "    'gemini-2.5-pro',\n",
    "]\n",
    "# 创建一个映射字典,将模型名称映射到排序索引\n",
    "model_order_map = {model: idx for idx, model in enumerate(model_order)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_mapping = {\n",
    "    \"blocksworld\": (0, 20),\n",
    "    \"frozenlake\": (0, 30),\n",
    "    \"sodoku\": (0, 20),\n",
    "    \"alfworld\": (0, 60),\n",
    "    \"webshop\": (0, 15)\n",
    "}\n",
    "from utils.analysis_files.analysis import get_config_label\n",
    "\n",
    "# Define all tasks\n",
    "all_tasks = [\"blocksworld\", \"frozenlake\", \"sodoku\", \"alfworld\", \"webshop\"]\n",
    "\n",
    "# Iterate through all tasks\n",
    "for task in all_tasks:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Set root directory for all experiment results\n",
    "    all_exp_base_dir = f\"res/{task}/\"\n",
    "    \n",
    "    # Load all experiment results into DataFrame\n",
    "    df_experiments = load_all_results(all_exp_base_dir)\n",
    "    \n",
    "    if df_experiments.empty:\n",
    "        print(f\"Failed to load experiment results for task {task}.\")\n",
    "        continue\n",
    "    \n",
    "    # Filter criteria\n",
    "    selected_df = df_experiments\n",
    "    if selection_criteria:\n",
    "        valid_criteria = {}\n",
    "        for key, value in selection_criteria.items():\n",
    "            if key in df_experiments.columns:\n",
    "                valid_criteria[key] = value\n",
    "        if valid_criteria:\n",
    "            query_str = \" & \".join([f\"`{k}` == {repr(v)}\" for k, v in valid_criteria.items()])\n",
    "            selected_df = df_experiments.query(query_str)\n",
    "    \n",
    "    # Sort\n",
    "    selected_df['model_sort_order'] = selected_df['model_name'].map(\n",
    "        lambda x: model_order_map.get(x, 999)\n",
    "    )\n",
    "    sort_list = ['enable_thinking','model_sort_order','chat_format','history_has_cot',\"state\"]\n",
    "    selected_df = selected_df.sort_values(by=sort_list, ascending=[True, True, True, True, False]).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    start, end = start_end_mapping.get(task)\n",
    "    selected_df = cal_area_under_curve(selected_df, start=start, end=end)\n",
    "    \n",
    "    # Print results\n",
    "    exclude_keys = set(selection_criteria.keys()) if selection_criteria else set()\n",
    "    print(\"\\nMEAN PASS\\tAUV\")\n",
    "    for idx, row in selected_df.iterrows():\n",
    "        pass_mean = row[\"pass_mean\"]\n",
    "        auv = row[\"pass1_auc\"]\n",
    "        print(f\"{pass_mean*100:.1f}\\t{auv*100:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in all_tasks:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Set root directory for all experiment results\n",
    "    all_exp_base_dir = f\"res/{task}/\"\n",
    "    \n",
    "    # Load all experiment results into DataFrame\n",
    "    df_experiments = load_all_results(all_exp_base_dir)\n",
    "    \n",
    "    if df_experiments.empty:\n",
    "        print(f\"Failed to load experiment results for task {task}.\")\n",
    "        continue\n",
    "    \n",
    "    # Filter criteria\n",
    "    selected_df = df_experiments\n",
    "    if selection_criteria:\n",
    "        valid_criteria = {}\n",
    "        for key, value in selection_criteria.items():\n",
    "            if key in df_experiments.columns:\n",
    "                valid_criteria[key] = value\n",
    "        if valid_criteria:\n",
    "            query_str = \" & \".join([f\"`{k}` == {repr(v)}\" for k, v in valid_criteria.items()])\n",
    "            selected_df = df_experiments.query(query_str)\n",
    "    \n",
    "    # Sort\n",
    "    selected_df['model_sort_order'] = selected_df['model_name'].map(\n",
    "        lambda x: model_order_map.get(x, 999)\n",
    "    )\n",
    "    sort_list = ['enable_thinking','model_sort_order','chat_format','history_has_cot',\"state\"]\n",
    "    selected_df = selected_df.sort_values(by=sort_list, ascending=[True, True, True, True, False]).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    start, end = start_end_mapping.get(task)\n",
    "    selected_df = cal_area_under_curve(selected_df, start=start, end=end)\n",
    "    \n",
    "    # Print results\n",
    "    exclude_keys = set(selection_criteria.keys()) if selection_criteria else set()\n",
    "    print(\"\\nAUV\\tLR\")\n",
    "    for idx, row in selected_df.iterrows():\n",
    "        loop_ratio_steplevel = row[\"mean_loop_ratio_after_invalid_steps_stepnorm\"]\n",
    "        # loop_ratio_trajlevel = row[\"mean_loop_ratio_after_invalid_steps_trajnorm\"]\n",
    "        auv = row[\"pass1_auc\"]\n",
    "        \n",
    "        print(f\"{auv*100:.1f}\\t{loop_ratio_steplevel*100:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
